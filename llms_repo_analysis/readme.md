# PR-Analysis with LLM ğŸš€

Dieses Projekt stellt eine Reihe von Befehlen zur Analyse und Bewertung von Pull Requests (PRs) mit lokal laufenden LLMs bereit. Zudem kÃ¶nnen die Ergebnisse mit DeepEval und menschlichem Feedback evaluiert werden.

## Setup

### 1. Repository klonen ğŸ”—
```bash
git clone git@git.i.mercedes-benz.com:LOBERNH/bachelor-thesis---LLMs-for-RepoAnalysis.git
```

### 2. In das Projektverzeichnis wechseln ğŸ“‚
```bash
cd llms_repo_analysis
```

### 3. Ollama installieren ğŸ¦™
[Ollama Download](https://ollama.com/download/)

### 4. Ollama-Modelle herunterladen ğŸ¦™
```bash
ollama pull deepseek-coder:6.7b
```

### 5. Ollama-Server starten ğŸ¦™
```bash
ollama serve
```

### 6. Datenbank erstellen ğŸ—„ï¸
```bash
python src/main.py create-db
```

### 7. AbhÃ¤ngigkeiten installieren ğŸ“¦
```bash
pip install -r requirements.txt
```

## PR-Beschreibung generieren und Code-Review mit Ollama durchfÃ¼hren

### 1. PR-Beschreibung generieren ğŸ“
Erzeugt eine Beschreibung fÃ¼r einen Pull Request.
```bash
python src/main.py generate-pr-description --pullrequest <PR_NUMBER> --repo <REPO> [--enterprise]
```
**Optionen:**
- `--pullrequest, -pr` : Die Nummer des Pull Requests.
- `--repo, -r` : Das Repository im Format `owner/repo`.
- `--enterprise, -e` : Flag, das angibt, ob das Repository ein Enterprise-Repository ist. Standard: `False`.

---

### 2. Code-Review hinzufÃ¼gen ğŸ’¬
FÃ¼gt Code-Review-Kommentare zu einem spezifischen Pull Request hinzu.
```bash
python src/main.py add-code-review --pullrequest <PR_NUMBER> --repo <REPO> [--enterprise]
```
**Optionen:**
- `--pullrequest, -pr` : Die Nummer des Pull Requests.
- `--repo, -r` : Das Repository im Format `owner/repo`.
- `--enterprise, -e` : Flag, das angibt, ob das Repository ein Enterprise-Repository ist. Standard: `False`.

---

## Generierte Ergebnisse evaluieren

### 1. Modell fÃ¼r DeepEval setzen ğŸ§ 
```bash
deepeval set-ollama deepseek-coder:6.7b
```
Oder mit einer benutzerdefinierten Basis-URL:
```bash
deepeval set-ollama deepseek-coder:6.7b --base-url="http://localhost:11434/v1/"
```

### 2. Evaluation mit menschlichem Feedback ğŸ§‘â€ğŸ’»
Startet eine Streamlit-Anwendung zur manuellen Bewertung mit menschlichem Feedback.
```bash
python eval/evaluate.py humanfeedback
```

### 3. Evaluation mit DeepEval ğŸ“Š
```bash
python eval/evaluate.py deepeval
```

### 4. Bericht generieren *(Coming Soon)* ğŸ“ˆ
Erzeugt einen Bericht.
```bash
python eval/evaluate.py report
```
